{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "# import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train df\n",
    "\n",
    "Загрузка `train_df` без признака `row_id` - дублирует индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "            'timestamp': 'int64',\n",
    "            'user_id': 'int32',\n",
    "            'content_id': 'int16',\n",
    "            'content_type_id': 'int8',\n",
    "            'task_container_id': 'int16',\n",
    "            'user_answer': 'int8',\n",
    "            'answered_correctly':'int8',\n",
    "            'prior_question_elapsed_time': 'float32',\n",
    "            'prior_question_had_explanation': 'object'\n",
    "            }\n",
    "train_path = '/home/ksu/Desktop/magistr/homework/HW_2/data/train.csv'\n",
    "train_df = pd.read_csv(train_path, usecols=[1,2,3,4,5,6,7,8,9], dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чистка данных\n",
    "- убираются пустые ячейки (строка 1)\n",
    "- признак `prior_question_had_explanation` преобразуется в `int8` (строки 2 - 6)\n",
    "- признак `prior_question_elapsed_time` преобразуется в секунды. Так как после этого данные укладываются в 2 байта\n",
    "признак преобразуется в `int16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_question_elapsed_time min value: 0.0 max value 300.0\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 98878794 entries, 1 to 101230331\n",
      "Data columns (total 9 columns):\n",
      "timestamp                         98878794 non-null int64\n",
      "user_id                           98878794 non-null int32\n",
      "content_id                        98878794 non-null int16\n",
      "content_type_id                   98878794 non-null int8\n",
      "task_container_id                 98878794 non-null int16\n",
      "user_answer                       98878794 non-null int8\n",
      "answered_correctly                98878794 non-null int8\n",
      "prior_question_elapsed_time       98878794 non-null int16\n",
      "prior_question_had_explanation    98878794 non-null int8\n",
      "dtypes: int16(3), int32(1), int64(1), int8(4)\n",
      "memory usage: 2.8 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df['prior_question_had_explanation'] = (\n",
    "    train_df['prior_question_had_explanation'].\n",
    "    apply(lambda has_explanation: 1 if has_explanation == 'True' else 0)\n",
    ")\n",
    "train_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype('int8')\n",
    "# now prior_question_elapsed_time will be in seconds\n",
    "train_df['prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'] / 1e3\n",
    "print('prior_question_elapsed_time min value: {} max value {}\\n'.format(\n",
    "                train_df['prior_question_elapsed_time'].min(),\n",
    "                train_df['prior_question_elapsed_time'].max()))\n",
    "train_df['prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'].astype('int16')\n",
    "train_df.info(memory_usage=200, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percents(dlen, flen):\n",
    "    return 100 * flen / dlen\n",
    "\n",
    "dlen = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timestamp\n",
    "\n",
    "the time in milliseconds between this user interaction and the first event completion from that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp max 1011.9days min 0.0s mode 0.0s\n",
      "Zero timestamp (or interaction = event, or garbage) amount 3896, in percents: 0.00%\n",
      "Timestamps which are more than 1 hour amount 90069811, in percents: 91.1%\n",
      "Timestamps which are more than 1 day amount 86039183, in percents: 87.0%\n"
     ]
    }
   ],
   "source": [
    "timestamp_secs = train_df['timestamp']/1e3\n",
    "print('Timestamp max {:.1f}days min {:.1f}s mode {:.1f}s'.format(timestamp_secs.max()/(3600 * 24),\n",
    "                                                                    timestamp_secs.min(),\n",
    "                                                                    timestamp_secs.mode()[0]))\n",
    "zero_ts_len = len(timestamp_secs[timestamp_secs == 0])\n",
    "hour_ts_len = len(timestamp_secs[timestamp_secs > 3600])\n",
    "day_ts_len = len(timestamp_secs[timestamp_secs > 3600 * 24])\n",
    "print('Zero timestamp (or interaction = event, or garbage) amount {}, in percents: {:.2f}%'.format(zero_ts_len, calc_percents(dlen, zero_ts_len)))\n",
    "print('Timestamps which are more than 1 hour amount {}, in percents: {:.1f}%'.format(hour_ts_len, calc_percents(dlen, hour_ts_len)))\n",
    "print('Timestamps which are more than 1 day amount {}, in percents: {:.1f}%'.format(day_ts_len, calc_percents(dlen, day_ts_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_id\n",
    "\n",
    "ID code for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of unquie users 393569, percents of df length 0.4%\n"
     ]
    }
   ],
   "source": [
    "user_id_unique = len(train_df['user_id'].unique())\n",
    "print('Amount of unquie users {}, percents of df length {:.1f}%'.format(user_id_unique, calc_percents(dlen, user_id_unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content_id\n",
    "\n",
    "ID code for the user interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of unquie content ids 13523, percents of df length 0.01%\n"
     ]
    }
   ],
   "source": [
    "content_id_len = len(train_df['content_id'].unique())\n",
    "print('Amount of unquie content ids {}, percents of df length {:.2f}%'.format(content_id_len, calc_percents(dlen, content_id_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task_container_id\n",
    "\n",
    "Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n",
    "\n",
    "Questions topics (themes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of unquie task containers ids 10000, percents of df length 0.01%\n"
     ]
    }
   ],
   "source": [
    "task_container_id_len = len(train_df['task_container_id'].unique())\n",
    "print('Amount of unquie task containers ids {}, percents of df length {:.2f}%'.format(\n",
    "    task_container_id_len,\n",
    "    calc_percents(dlen, task_container_id_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content_type_id\n",
    "\n",
    "0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: content_type_id, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['content_type_id'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all content is questions, so this data is not usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56943</td>\n",
       "      <td>115</td>\n",
       "      <td>5716</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118363</td>\n",
       "      <td>115</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131167</td>\n",
       "      <td>115</td>\n",
       "      <td>7860</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137965</td>\n",
       "      <td>115</td>\n",
       "      <td>7922</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157063</td>\n",
       "      <td>115</td>\n",
       "      <td>156</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  user_answer  \\\n",
       "1      56943      115        5716                  2            2   \n",
       "2     118363      115         128                  0            0   \n",
       "3     131167      115        7860                  3            0   \n",
       "4     137965      115        7922                  4            1   \n",
       "5     157063      115         156                  5            2   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "1                   1                           37   \n",
       "2                   1                           55   \n",
       "3                   1                           19   \n",
       "4                   1                           11   \n",
       "5                   1                            5   \n",
       "\n",
       "   prior_question_had_explanation  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               0  \n",
       "5                               0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(['content_type_id'], axis=1, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_answer\n",
    "\n",
    "the user's answer to the question, if any. Read -1 as null, for lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27989383\n",
       "1    26912164\n",
       "3    26025115\n",
       "2    17952132\n",
       "Name: user_answer, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_answer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answered_correctly\n",
    "\n",
    "if the user responded correctly. Read -1 as null, for lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    64977687\n",
       "0    33901107\n",
       "Name: answered_correctly, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['answered_correctly'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior_question_had_explanation\n",
    "\n",
    "Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    89685560\n",
       "0     9193234\n",
       "Name: prior_question_had_explanation, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['prior_question_had_explanation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior_question_elapsed_time\n",
    "\n",
    " The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time min: 0 max: 300 mode: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17     5485415\n",
       "16     5061745\n",
       "18     5022869\n",
       "19     4412104\n",
       "20     4063010\n",
       "15     3924757\n",
       "21     3898864\n",
       "22     3763852\n",
       "23     3520538\n",
       "24     3231931\n",
       "25     2931446\n",
       "14     2885152\n",
       "26     2635191\n",
       "27     2349886\n",
       "13     2275213\n",
       "28     2088975\n",
       "12     1991117\n",
       "29     1878755\n",
       "11     1780508\n",
       "30     1705451\n",
       "10     1673832\n",
       "9      1634798\n",
       "31     1544286\n",
       "8      1478082\n",
       "32     1407656\n",
       "33     1293700\n",
       "7      1264454\n",
       "34     1187995\n",
       "35     1089025\n",
       "6      1087464\n",
       "        ...   \n",
       "268        585\n",
       "287        575\n",
       "271        574\n",
       "274        571\n",
       "277        570\n",
       "273        569\n",
       "282        561\n",
       "270        549\n",
       "280        528\n",
       "272        525\n",
       "281        522\n",
       "276        520\n",
       "278        513\n",
       "283        501\n",
       "289        495\n",
       "290        492\n",
       "284        491\n",
       "296        489\n",
       "288        487\n",
       "295        483\n",
       "279        481\n",
       "294        469\n",
       "291        467\n",
       "292        464\n",
       "297        448\n",
       "299        447\n",
       "293        445\n",
       "286        443\n",
       "298        439\n",
       "285        420\n",
       "Name: prior_question_elapsed_time, Length: 301, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Elapsed time min: {} max: {} mode: {}'.format(train_df['prior_question_elapsed_time'].min(),\n",
    "                                                  train_df['prior_question_elapsed_time'].max(),\n",
    "                                                  train_df['prior_question_elapsed_time'].mode()[0]))\n",
    "train_df['prior_question_elapsed_time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading questions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13523 entries, 0 to 13522\n",
      "Data columns (total 5 columns):\n",
      "question_id       13523 non-null int64\n",
      "bundle_id         13523 non-null int64\n",
      "correct_answer    13523 non-null int64\n",
      "part              13523 non-null int64\n",
      "tags              13522 non-null object\n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 528.3+ KB\n"
     ]
    }
   ],
   "source": [
    "questions_df = pd.read_csv('/home/ksu/Desktop/magistr/homework/HW_2/data/questions.csv')\n",
    "questions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question_id\n",
    "\n",
    "foreign key for the train/test content_id column, when the content type is question (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "Name: question_id, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['question_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51 131 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>131 36 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 101 162 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 149 162 29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>131 5 162 38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bundle_id  correct_answer  part            tags\n",
       "0          0               0     1   51 131 162 38\n",
       "1          1               1     1       131 36 81\n",
       "2          2               0     1  131 101 162 92\n",
       "3          3               0     1  131 149 162 29\n",
       "4          4               3     1    131 5 162 38"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.drop(['question_id'], inplace=True, axis=1)\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bundle_id\n",
    "\n",
    "code for which questions are served together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7795    5\n",
       "6971    5\n",
       "7421    5\n",
       "7770    5\n",
       "8144    5\n",
       "Name: bundle_id, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['bundle_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct_answer\n",
    "\n",
    "the answer to the question. Can be compared with the train user_answer column to check if the user was right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3716\n",
       "3    3544\n",
       "1    3478\n",
       "2    2785\n",
       "Name: correct_answer, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['correct_answer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part\n",
    "\n",
    "the relevant section of the TOEIC test.\n",
    "\n",
    "https://www.iibc-global.org/english/toeic/test/lr/about/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5511\n",
       "2    1647\n",
       "3    1562\n",
       "4    1439\n",
       "6    1212\n",
       "7    1160\n",
       "1     992\n",
       "Name: part, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['part'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag\n",
    "\n",
    "one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     51 131 162 38\n",
       "1         131 36 81\n",
       "2    131 101 162 92\n",
       "3    131 149 162 29\n",
       "4      131 5 162 38\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['tags'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading lectures data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 4 columns):\n",
      "lecture_id    418 non-null int64\n",
      "tag           418 non-null int64\n",
      "part          418 non-null int64\n",
      "type_of       418 non-null object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 13.1+ KB\n"
     ]
    }
   ],
   "source": [
    "lectures_df = pd.read_csv('/home/ksu/Desktop/magistr/homework/HW_2/data/lectures.csv')\n",
    "lectures_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type_of\n",
    "\n",
    "brief description of the core purpose of the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept             222\n",
       "solving question    186\n",
       "intention             7\n",
       "starter               3\n",
       "Name: type_of, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df['type_of'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part\n",
    "\n",
    "top level category code for the lecture.\n",
    "\n",
    "https://www.iibc-global.org/english/toeic/test/lr/about/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    143\n",
       "6     83\n",
       "2     56\n",
       "1     54\n",
       "7     32\n",
       "4     31\n",
       "3     19\n",
       "Name: part, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df['part'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag\n",
    "\n",
    "one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    159\n",
       "1     70\n",
       "2     45\n",
       "3     79\n",
       "4    156\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df['tag'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
