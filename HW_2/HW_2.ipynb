{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "# import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train df\n",
    "\n",
    "Загрузка `train_df` без признака `row_id` - дублирует индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10 ** 5\n",
    "dtypes = {\n",
    "            'timestamp': 'int64',\n",
    "            'user_id': 'int32',\n",
    "            'content_id': 'int16',\n",
    "            'content_type_id': 'int8',\n",
    "            'task_container_id': 'int16',\n",
    "            'user_answer': 'int8',\n",
    "            'answered_correctly':'int8',\n",
    "            'prior_question_elapsed_time': 'float32',\n",
    "            'prior_question_had_explanation': 'object'\n",
    "            }\n",
    "train_path = '/home/ksu/Desktop/magistr/homework/HW_2/data/train.csv'\n",
    "if chunksize:\n",
    "    chunks = pd.read_csv(train_path, usecols=[1,2,3,4,5,6,7,8,9],\n",
    "                         dtype=dtypes, chunksize=chunksize, low_memory=False)\n",
    "    train_df = pd.DataFrame(chunks.get_chunk(chunksize))\n",
    "    # df=pd.concat(chunk for chunk in chunks)\n",
    "else:\n",
    "    train_df = pd.read_csv(train_path, usecols=[1,2,3,4,5,6,7,8,9], dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "- убираются пустые ячейки (строка 1)\n",
    "- признак `prior_question_had_explanation` преобразуется в `int8` (строки 2 - 6)\n",
    "- признак `prior_question_elapsed_time` преобразуется в секунды. Так как после этого данные укладываются в 2 байта\n",
    "признак преобразуется в `int16`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_question_elapsed_time min value: 0.0 max value 300.0\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97834 entries, 1 to 99999\n",
      "Data columns (total 9 columns):\n",
      "timestamp                         97834 non-null int64\n",
      "user_id                           97834 non-null int32\n",
      "content_id                        97834 non-null int16\n",
      "content_type_id                   97834 non-null int8\n",
      "task_container_id                 97834 non-null int16\n",
      "user_answer                       97834 non-null int8\n",
      "answered_correctly                97834 non-null int8\n",
      "prior_question_elapsed_time       97834 non-null int16\n",
      "prior_question_had_explanation    97834 non-null int8\n",
      "dtypes: int16(3), int32(1), int64(1), int8(4)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df['prior_question_had_explanation'] = (\n",
    "    train_df['prior_question_had_explanation'].\n",
    "    apply(lambda has_explanation: 1 if has_explanation == 'True' else 0)\n",
    ")\n",
    "train_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype('int8')\n",
    "# now prior_question_elapsed_time will be in seconds\n",
    "train_df['prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'] / 1e3\n",
    "print('prior_question_elapsed_time min value: {} max value {}\\n'.format(\n",
    "                train_df['prior_question_elapsed_time'].min(),\n",
    "                train_df['prior_question_elapsed_time'].max()))\n",
    "train_df['prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'].astype('int16')\n",
    "train_df.info(memory_usage=200, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading questions and lectures data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv('/home/ksu/Desktop/magistr/homework/HW_2/data/questions.csv')\n",
    "lectures_df = pd.read_csv('/home/ksu/Desktop/magistr/homework/HW_2/data/lectures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97834 entries, 0 to 97833\n",
      "Data columns (total 18 columns):\n",
      "timestamp                         97834 non-null int64\n",
      "user_id                           97834 non-null int32\n",
      "content_id                        97834 non-null int16\n",
      "content_type_id                   97834 non-null int8\n",
      "task_container_id                 97834 non-null int16\n",
      "user_answer                       97834 non-null int8\n",
      "answered_correctly                97834 non-null int8\n",
      "prior_question_elapsed_time       97834 non-null int16\n",
      "prior_question_had_explanation    97834 non-null int8\n",
      "question_id                       97834 non-null int64\n",
      "bundle_id                         97834 non-null int64\n",
      "correct_answer                    97834 non-null int64\n",
      "part_x                            97834 non-null int64\n",
      "tags                              97834 non-null object\n",
      "lecture_id                        1244 non-null float64\n",
      "tag                               1244 non-null float64\n",
      "part_y                            1244 non-null float64\n",
      "type_of                           1244 non-null object\n",
      "dtypes: float64(3), int16(3), int32(1), int64(5), int8(4), object(2)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.merge(train_df, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "train = pd.merge(train, lectures_df, left_on = 'content_id', right_on = 'lecture_id', how = 'left')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percents(dlen, flen):\n",
    "    return 100 * flen / dlen\n",
    "\n",
    "dlen = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timestamp\n",
    "\n",
    "the time in milliseconds between this user interaction and the first event completion from that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp max 688.6days min 0.0s mode 341.4s\n",
      "Zero timestamp (or interaction = event, or garbage) amount 4, in percents: 0.00%\n",
      "Timestamps which are more than 1 hour amount 89816, in percents: 91.8%\n",
      "Timestamps which are more than 1 day amount 85640, in percents: 87.5%\n"
     ]
    }
   ],
   "source": [
    "timestamp_secs = train_df['timestamp']/1e3\n",
    "print('Timestamp max {:.1f}days min {:.1f}s mode {:.1f}s'.format(timestamp_secs.max()/(3600 * 24),\n",
    "                                                                    timestamp_secs.min(),\n",
    "                                                                    timestamp_secs.mode()[0]))\n",
    "zero_ts_len = len(timestamp_secs[timestamp_secs == 0])\n",
    "hour_ts_len = len(timestamp_secs[timestamp_secs > 3600])\n",
    "day_ts_len = len(timestamp_secs[timestamp_secs > 3600 * 24])\n",
    "print('Zero timestamp (or interaction = event, or garbage) amount {}, in percents: {:.2f}%'.format(zero_ts_len, calc_percents(dlen, zero_ts_len)))\n",
    "print('Timestamps which are more than 1 hour amount {}, in percents: {:.1f}%'.format(hour_ts_len, calc_percents(dlen, hour_ts_len)))\n",
    "print('Timestamps which are more than 1 day amount {}, in percents: {:.1f}%'.format(day_ts_len, calc_percents(dlen, day_ts_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_id\n",
    "\n",
    "ID code for the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of unquie users 348, percents of df length 0.4%\n"
     ]
    }
   ],
   "source": [
    "user_id_unique = len(train_df['user_id'].unique())\n",
    "print('Amount of unquie users {}, percents of df length {:.1f}%'.format(user_id_unique, calc_percents(dlen, user_id_unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content_id\n",
    "\n",
    "ID code for the user interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of unquie content ids 11308, percents of df length 11.56%\n"
     ]
    }
   ],
   "source": [
    "content_id_len = len(train_df['content_id'].unique())\n",
    "print('Amount of unquie content ids {}, percents of df length {:.2f}%'.format(content_id_len, calc_percents(dlen, content_id_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task_container_id\n",
    "\n",
    "Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n",
    "\n",
    "Questions topics (themes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of unquie task containers ids 5631, percents of df length 5.76%\n"
     ]
    }
   ],
   "source": [
    "task_container_id_len = len(train_df['task_container_id'].unique())\n",
    "print('Amount of unquie task containers ids {}, percents of df length {:.2f}%'.format(\n",
    "    task_container_id_len,\n",
    "    calc_percents(dlen, task_container_id_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content_type_id\n",
    "\n",
    "0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: content_type_id, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['content_type_id'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all content is questions, so this data is not usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56943</td>\n",
       "      <td>115</td>\n",
       "      <td>5716</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118363</td>\n",
       "      <td>115</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131167</td>\n",
       "      <td>115</td>\n",
       "      <td>7860</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137965</td>\n",
       "      <td>115</td>\n",
       "      <td>7922</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157063</td>\n",
       "      <td>115</td>\n",
       "      <td>156</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  user_id  content_id  task_container_id  user_answer  \\\n",
       "1      56943      115        5716                  2            2   \n",
       "2     118363      115         128                  0            0   \n",
       "3     131167      115        7860                  3            0   \n",
       "4     137965      115        7922                  4            1   \n",
       "5     157063      115         156                  5            2   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "1                   1                           37   \n",
       "2                   1                           55   \n",
       "3                   1                           19   \n",
       "4                   1                           11   \n",
       "5                   1                            5   \n",
       "\n",
       "   prior_question_had_explanation  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               0  \n",
       "5                               0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(['content_type_id'], axis=1, inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_answer\n",
    "\n",
    "the user's answer to the question, if any. Read -1 as null, for lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27759\n",
       "1    26428\n",
       "3    25984\n",
       "2    17663\n",
       "Name: user_answer, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_answer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answered_correctly\n",
    "\n",
    "if the user responded correctly. Read -1 as null, for lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    67100\n",
       "0    30734\n",
       "Name: answered_correctly, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['answered_correctly'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior_question_had_explanation\n",
    "\n",
    "Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    90349\n",
       "0     7485\n",
       "Name: prior_question_had_explanation, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['prior_question_had_explanation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prior_question_elapsed_time\n",
    "\n",
    " The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time min: 0 max: 300 mode: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17     5809\n",
       "16     5449\n",
       "18     5168\n",
       "19     4507\n",
       "20     4204\n",
       "15     4126\n",
       "22     4069\n",
       "21     4059\n",
       "23     3773\n",
       "24     3599\n",
       "14     3073\n",
       "25     3058\n",
       "26     2823\n",
       "27     2483\n",
       "13     2322\n",
       "12     2184\n",
       "28     2047\n",
       "29     1986\n",
       "11     1856\n",
       "31     1631\n",
       "10     1617\n",
       "30     1613\n",
       "9      1580\n",
       "32     1415\n",
       "8      1383\n",
       "33     1221\n",
       "34     1197\n",
       "7      1062\n",
       "35     1061\n",
       "6       898\n",
       "       ... \n",
       "170       1\n",
       "153       1\n",
       "189       1\n",
       "281       1\n",
       "190       1\n",
       "184       1\n",
       "151       1\n",
       "175       1\n",
       "201       1\n",
       "176       1\n",
       "241       1\n",
       "203       1\n",
       "275       1\n",
       "165       1\n",
       "266       1\n",
       "212       1\n",
       "228       1\n",
       "265       1\n",
       "244       1\n",
       "147       1\n",
       "269       1\n",
       "272       1\n",
       "213       1\n",
       "226       1\n",
       "209       1\n",
       "135       1\n",
       "239       1\n",
       "224       1\n",
       "262       1\n",
       "146       1\n",
       "Name: prior_question_elapsed_time, Length: 220, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Elapsed time min: {} max: {} mode: {}'.format(train_df['prior_question_elapsed_time'].min(),\n",
    "                                                  train_df['prior_question_elapsed_time'].max(),\n",
    "                                                  train_df['prior_question_elapsed_time'].mode()[0]))\n",
    "train_df['prior_question_elapsed_time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question_id\n",
    "\n",
    "foreign key for the train/test content_id column, when the content type is question (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    3\n",
       "4    4\n",
       "Name: question_id, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['question_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51 131 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>131 36 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 101 162 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 149 162 29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>131 5 162 38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bundle_id  correct_answer  part            tags\n",
       "0          0               0     1   51 131 162 38\n",
       "1          1               1     1       131 36 81\n",
       "2          2               0     1  131 101 162 92\n",
       "3          3               0     1  131 149 162 29\n",
       "4          4               3     1    131 5 162 38"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.drop(['question_id'], inplace=True, axis=1)\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bundle_id\n",
    "\n",
    "code for which questions are served together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7795    5\n",
       "6971    5\n",
       "7421    5\n",
       "7770    5\n",
       "8144    5\n",
       "Name: bundle_id, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['bundle_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct_answer\n",
    "\n",
    "the answer to the question. Can be compared with the train user_answer column to check if the user was right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3716\n",
       "3    3544\n",
       "1    3478\n",
       "2    2785\n",
       "Name: correct_answer, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['correct_answer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part\n",
    "\n",
    "the relevant section of the TOEIC test.\n",
    "\n",
    "https://www.iibc-global.org/english/toeic/test/lr/about/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    5511\n",
       "2    1647\n",
       "3    1562\n",
       "4    1439\n",
       "6    1212\n",
       "7    1160\n",
       "1     992\n",
       "Name: part, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['part'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag\n",
    "\n",
    "one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     51 131 162 38\n",
       "1         131 36 81\n",
       "2    131 101 162 92\n",
       "3    131 149 162 29\n",
       "4      131 5 162 38\n",
       "Name: tags, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df['tags'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type_of\n",
    "\n",
    "brief description of the core purpose of the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept             222\n",
       "solving question    186\n",
       "intention             7\n",
       "starter               3\n",
       "Name: type_of, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df['type_of'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part\n",
    "\n",
    "top level category code for the lecture.\n",
    "\n",
    "https://www.iibc-global.org/english/toeic/test/lr/about/format.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    143\n",
       "6     83\n",
       "2     56\n",
       "1     54\n",
       "7     32\n",
       "4     31\n",
       "3     19\n",
       "Name: part, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df['part'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag\n",
    "\n",
    "one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NaN\n",
       "1         NaN\n",
       "2         NaN\n",
       "3         NaN\n",
       "4         NaN\n",
       "5         NaN\n",
       "6         NaN\n",
       "7         NaN\n",
       "8         NaN\n",
       "9         NaN\n",
       "10        NaN\n",
       "11        NaN\n",
       "12        NaN\n",
       "13        NaN\n",
       "14        NaN\n",
       "15        NaN\n",
       "16        NaN\n",
       "17       57.0\n",
       "18        NaN\n",
       "19        NaN\n",
       "20        NaN\n",
       "21        NaN\n",
       "22       45.0\n",
       "23        NaN\n",
       "24        NaN\n",
       "25        NaN\n",
       "26        NaN\n",
       "27        NaN\n",
       "28        NaN\n",
       "29       70.0\n",
       "         ... \n",
       "97804     NaN\n",
       "97805     NaN\n",
       "97806     NaN\n",
       "97807     NaN\n",
       "97808     NaN\n",
       "97809     NaN\n",
       "97810     NaN\n",
       "97811     NaN\n",
       "97812     NaN\n",
       "97813     NaN\n",
       "97814     NaN\n",
       "97815     NaN\n",
       "97816     NaN\n",
       "97817     NaN\n",
       "97818     NaN\n",
       "97819     NaN\n",
       "97820     NaN\n",
       "97821     NaN\n",
       "97822     NaN\n",
       "97823     NaN\n",
       "97824     NaN\n",
       "97825     NaN\n",
       "97826     NaN\n",
       "97827     NaN\n",
       "97828     NaN\n",
       "97829     NaN\n",
       "97830     NaN\n",
       "97831     NaN\n",
       "97832     NaN\n",
       "97833     NaN\n",
       "Name: tag, Length: 97834, dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tag']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
